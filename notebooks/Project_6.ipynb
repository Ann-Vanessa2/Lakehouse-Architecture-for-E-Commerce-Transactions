{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHzxP31m4wXl"
      },
      "outputs": [],
      "source": [
        "# !pip install awscli\n",
        "!pip install --upgrade awscli botocore\n",
        "# !aws configure"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get install openjdk-11-jdk -y"
      ],
      "metadata": {
        "id": "NA3lxomq5pSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install boto3 pyspark"
      ],
      "metadata": {
        "id": "YN52mgu-5hgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk -y\n",
        "!pip install pyspark boto3\n",
        "!wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.jar\n",
        "!wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar"
      ],
      "metadata": {
        "id": "yOSsEjpb5sEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install delta-spark"
      ],
      "metadata": {
        "id": "92s12MMGQanu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "R4LmUgCiRa6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install delta-spark\n",
        "!pip install delta-spark==2.2.0\n",
        "\n",
        "# download the required jar files\n",
        "!wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.jar\n",
        "!wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar\n"
      ],
      "metadata": {
        "id": "TpCXG7l0dK0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars hadoop-aws-3.3.2.jar,aws-java-sdk-bundle-1.12.262.jar pyspark-shell'\n",
        "# set the environment variable to include the jar files to the spark session\n",
        "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars hadoop-aws-3.3.2.jar,aws-java-sdk-bundle-1.12.262.jar pyspark-shell'"
      ],
      "metadata": {
        "id": "fzAKHbi450d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "2pax2qocRO9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark.sql import SparkSession\n",
        "# from pyspark.sql.functions import when, col, count, countDistinct, sum, avg, row_number, to_date, date_format, dense_rank, desc, trim, datediff, min, max, unix_timestamp, to_timestamp, round\n",
        "from pyspark.sql.functions import when, col, count\n",
        "from pyspark.sql.window import Window\n",
        "import boto3\n",
        "import pyspark.sql.functions as F\n",
        "from delta.tables import DeltaTable"
      ],
      "metadata": {
        "id": "O_phA7Zo52sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "AWS_ACCESS_KEY = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "AWS_SECRET_KEY = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "\n",
        "# Verify if credentials are loaded\n",
        "print(f\"AWS_ACCESS_KEY: {AWS_ACCESS_KEY[:4]}********\")"
      ],
      "metadata": {
        "id": "a9Ez888I54Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session with S3 support\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"LakehouseEcommerce\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY) \\\n",
        "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY) \\\n",
        "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.2.0,org.apache.hadoop:hadoop-aws:3.3.2,org.apache.hadoop:hadoop-common:3.3.2\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .config(\"spark.databricks.delta.schema.autoMerge.enable\", \"true\") \\\n",
        "    .config(\"spark.jars\", \"https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.4.0/delta-core_2.12-2.4.0.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# spark = SparkSession.builder.appName(\"VehicleLocationMetrics\").getOrCreate()"
      ],
      "metadata": {
        "id": "m07-kACd5_Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize S3 Hook\n",
        "# s3_hook = S3Hook(aws_conn_id=\"aws_default\")\n",
        "\n",
        "S3_BUCKET_NAME = \"lakehouse-e-commerce\"\n",
        "AWS_REGION = \"eu-west-1\"\n",
        "DATA_FOLDER = \"raw-data/\"\n",
        "PROCESSED_DATA_FOLDER = \"processed/\"\n",
        "ARCHIVED_DATA_FOLDER = \"archive/\"\n",
        "DELTA_LAKE_FOLDER = \"lakehouse-dwh/\""
      ],
      "metadata": {
        "id": "2EbixBLG6B3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s3 = boto3.client('s3',\n",
        "                  aws_access_key_id=AWS_ACCESS_KEY,\n",
        "                  aws_secret_access_key=AWS_SECRET_KEY,\n",
        "                  region_name=AWS_REGION)"
      ],
      "metadata": {
        "id": "YA8-Il7j7z6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging sheets in orders.xlsx and order_items.xlsx"
      ],
      "metadata": {
        "id": "WfXA70NarC_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the xlsx files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# loading xlsx file through file path\n",
        "# orders_df = pd.read_excel('orders_apr_2025.xlsx')\n",
        "# order_items_df = pd.read_excel('order_items_apr_2025.xlsx')"
      ],
      "metadata": {
        "id": "0UfZxqzmWtLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combining the sheets into one dataframe\n",
        "import pandas as pd\n",
        "\n",
        "def read_all_sheets(xlsx_file):\n",
        "    # Read the Excel file\n",
        "    xls = pd.ExcelFile(xlsx_file)\n",
        "    sheets = xls.sheet_names\n",
        "    combined_df = pd.concat([xls.parse(sheet) for sheet in sheets], ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "orders_df = read_all_sheets('orders_apr_2025.xlsx')\n",
        "order_items_df = read_all_sheets('order_items_apr_2025.xlsx')"
      ],
      "metadata": {
        "id": "wzV5sdTdXIEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df.tail()"
      ],
      "metadata": {
        "id": "4WzjXiqkYYl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving dataframes in csv files in colab\n",
        "orders_df.to_csv('orders.csv', index=False)\n",
        "order_items_df.to_csv('order_items.csv', index=False)"
      ],
      "metadata": {
        "id": "VuI16U--Gc9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "\n",
        "def upload_df_to_s3(df, bucket, key):\n",
        "    csv_buffer = StringIO()\n",
        "    df.to_csv(csv_buffer, index=False)\n",
        "    s3.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())"
      ],
      "metadata": {
        "id": "sX-gqO8Zacrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upload_df_to_s3(orders_df, S3_BUCKET_NAME, \"raw-data/orders.csv\")\n",
        "upload_df_to_s3(order_items_df, S3_BUCKET_NAME, \"raw-data/order_items.csv\")"
      ],
      "metadata": {
        "id": "6tuq7a6Eapkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Continuation"
      ],
      "metadata": {
        "id": "wklbcs8grQvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # List files in bucket\n",
        "# response = s3.list_objects_v2(Bucket=S3_BUCKET_NAME)\n",
        "# for obj in response.get('Contents', []):\n",
        "#     print(obj['Key'])  # Printing the file names"
      ],
      "metadata": {
        "id": "M0CxvdUrrg7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining S3 folder paths\n",
        "products_path = f\"s3a://{S3_BUCKET_NAME}/{DATA_FOLDER}/products.csv\"\n",
        "orders_path = f\"s3a://{S3_BUCKET_NAME}/{DATA_FOLDER}/orders.csv\"\n",
        "order_items_path = f\"s3a://{S3_BUCKET_NAME}/{DATA_FOLDER}/order_items.csv\"\n",
        "products_lakehouse_path = f\"s3a://{S3_BUCKET_NAME}/{DELTA_LAKE_FOLDER}/products\"\n",
        "orders_lakehouse_path = f\"s3a://{S3_BUCKET_NAME}/{DELTA_LAKE_FOLDER}/orders\"\n",
        "order_items_lakehouse_path = f\"s3a://{S3_BUCKET_NAME}/{DELTA_LAKE_FOLDER}/order_items\""
      ],
      "metadata": {
        "id": "Ja3p367p7xHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data from S3\n",
        "products_df = spark.read.option(\"header\", True).csv(products_path)\n",
        "orders_df = spark.read.option(\"header\", True).csv(orders_path)\n",
        "order_items_df = spark.read.option(\"header\", True).csv(order_items_path)"
      ],
      "metadata": {
        "id": "7qTt5frF-1ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products_df.show()\n",
        "orders_df.show()\n",
        "order_items_df.show()"
      ],
      "metadata": {
        "id": "gPDLzYHF8GvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products_df.printSchema()\n",
        "orders_df.printSchema()\n",
        "order_items_df.printSchema()"
      ],
      "metadata": {
        "id": "DJK3xnfF_3CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products_df.filter(col(\"product_id\").isNull()).show()"
      ],
      "metadata": {
        "id": "GNVaXNX3loLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df.filter(col(\"order_timestamp\").isNull()).show()"
      ],
      "metadata": {
        "id": "9KVwz3mtHyq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for missing values in each column\n",
        "orders_df.select([count(when(col(c).isNull(), c)).alias(c) for c in orders_df.columns]).show()"
      ],
      "metadata": {
        "id": "-9WYMdcernHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_items_df.select([count(when(col(c).isNull(), c)).alias(c) for c in order_items_df.columns]).show()"
      ],
      "metadata": {
        "id": "NG3sey7csBDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products_df.select([count(when(col(c).isNull(), c)).alias(c) for c in products_df.columns]).show()"
      ],
      "metadata": {
        "id": "arn-VvdBsJpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Glue Jobs"
      ],
      "metadata": {
        "id": "i8uf4ADfDE5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Common helper functions\n",
        "def validate_schema(df, required_columns):\n",
        "    missing = [col for col in required_columns if col not in df.columns]\n",
        "    if missing:\n",
        "        raise Exception(f\"Missing columns: {missing}\")\n",
        "    return df\n",
        "\n",
        "def deduplicate(df, primary_keys):\n",
        "    return df.dropDuplicates(primary_keys)\n",
        "\n",
        "def validate_nulls(df, columns):\n",
        "    for col in columns:\n",
        "        for null_count in df.select(F.count(F.when(F.col(col).isNull(), col))).collect()[0]:\n",
        "            if null_count > 0:\n",
        "                raise Exception(f\"Nulls found in column {col}\")\n",
        "    return df\n",
        "\n",
        "def write_delta(df, output_path, partition_by=None):\n",
        "    if partition_by:\n",
        "        df.write.format(\"delta\").mode(\"overwrite\").partitionBy(partition_by).save(output_path)\n",
        "    else:\n",
        "        df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
        "\n",
        "def merge_upsert(spark, delta_path, df, primary_keys):\n",
        "    from delta.tables import DeltaTable\n",
        "    delta_table = DeltaTable.forPath(spark, delta_path)\n",
        "    cond = ' AND '.join([f\"target.{pk} = source.{pk}\" for pk in primary_keys])\n",
        "\n",
        "    delta_table.alias(\"target\").merge(\n",
        "        source=df.alias(\"source\"),\n",
        "        condition=cond\n",
        "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n"
      ],
      "metadata": {
        "id": "xnXG8yFhh_sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Products ETL Job ---\n",
        "\n",
        "# Read from S3\n",
        "p_df = spark.read.csv(products_path, header=True)\n",
        "\n",
        "# Validate schema\n",
        "required_columns = [\"product_id\", \"department_id\", \"department\", \"product_name\"]\n",
        "p_df = validate_schema(p_df, required_columns)\n",
        "\n",
        "# Check for null primary keys\n",
        "p_df = validate_nulls(p_df, [\"product_id\"])\n",
        "\n",
        "# Deduplicate\n",
        "p_df = deduplicate(p_df, [\"product_id\"])\n",
        "\n",
        "# # Ordering by product_id before writing to the delta lake tables\n",
        "# p_df = p_df.orderBy(\"product_id\")\n",
        "\n",
        "# Write to Delta\n",
        "# write_delta(p_df, products_lakehouse_path, partition_by=\"department_id\")\n",
        "\n",
        "print(\"Products ETL job finished successfully.\")"
      ],
      "metadata": {
        "id": "XVdvfwVxiT5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_delta(p_df, products_lakehouse_path, partition_by=\"department_id\")"
      ],
      "metadata": {
        "id": "qq31W9Lxpvq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_df.show(50)"
      ],
      "metadata": {
        "id": "vSNiKg9Bo_u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show values with product_id of 2\n",
        "p_df.filter(col(\"product_id\") == 3).show()"
      ],
      "metadata": {
        "id": "wEHnIwplMcl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Orders ETL Job ---\n",
        "\n",
        "# Read from S3\n",
        "o_df = spark.read.csv(orders_path, header=True)\n",
        "\n",
        "# Validate schema\n",
        "required_columns = [\"order_num\", \"order_id\", \"user_id\", \"order_timestamp\", \"total_amount\", \"date\"]\n",
        "o_df = validate_schema(o_df, required_columns)\n",
        "\n",
        "# Check for null primary keys\n",
        "o_df = validate_nulls(o_df, [\"order_id\"])\n",
        "\n",
        "# Deduplicate\n",
        "o_df = deduplicate(o_df, [\"order_id\"])\n",
        "\n",
        "# Write to Delta\n",
        "write_delta(o_df, orders_lakehouse_path, partition_by=\"date\")\n",
        "\n",
        "print(\"Products ETL job finished successfully.\")"
      ],
      "metadata": {
        "id": "o0pjBo2oo1Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Order_items ETL Job ---\n",
        "\n",
        "# Read from S3\n",
        "oi_df = spark.read.csv(order_items_path, header=True)\n",
        "\n",
        "# Validate schema\n",
        "required_columns = [\"id\", \"order_id\", \"user_id\", \"days_since_prior_order\", \"product_id\", \"add_to_cart_order\", \"reordered\", \"order_timestamp\", \"date\"]\n",
        "oi_df = validate_schema(oi_df, required_columns)\n",
        "\n",
        "# Check for null primary keys\n",
        "oi_df = validate_nulls(oi_df, [\"id\"])\n",
        "\n",
        "# Deduplicate\n",
        "oi_df = deduplicate(oi_df, [\"id\"])\n",
        "\n",
        "# Write to Delta\n",
        "write_delta(oi_df, order_items_lakehouse_path, partition_by=\"date\")\n",
        "\n",
        "print(\"Products ETL job finished successfully.\")"
      ],
      "metadata": {
        "id": "-eQBw4ozuNBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_delta(oi_df, order_items_lakehouse_path, partition_by=\"date\")"
      ],
      "metadata": {
        "id": "muyv25Nb6SKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22idVYZM8LOR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}